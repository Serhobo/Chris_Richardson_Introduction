---
title: Welcome back, I got you Flowers.
subtitle: Please see the below link to my second contribution to a Kaggle Compettion. I was able to achieve a higher accurracy from our tutorial code by creating a train / dev / test system, and trialing different hyperparamets. Take a look for yourself!

# Summary for listings and search engines
summary: Please see the below link to my second contribution to a Kaggle Compettion. I was able to achieve a higher accurracy from our tutorial code by creating a train / dev / test system, and trialing different hyperparamets. Take a look for yourself!

# Link this post with a project
projects: [Personal Project]

# Date published
date: "2022-3-25T19:54:00Z"

# Date updated
lastmod: "2022-3-25T19:54:00Z"

# Is this an unpublished draft?
draft: false

# Show this page in the Featured widget?
featured: true

# Featured image
# Place an image named `featured.jpg/png` in this page's folder and customize its options here.
image:
  caption: 'Image credit: [**Unsplash**](https://storage.googleapis.com/kaggle-datasets-images/8782/12270/c3af536d14e386a2bfd356d1cd84b67e/dataset-cover.jpg?t=2018-01-06-14-10-54)'
  focal_point: ""
  placement: 2
  preview_only: false

authors:
- admin

tags:
- Academic
- NN
- ML

categories:
- Demo
- Personal Project
---

## Overview

https://github.com/Serhobo/Project-Preliminary


## Contributions

I tested adding and swapping out the variables originally used within the creation of the decision trees. Additionally, I tested various random states, as well as number of estimators and depth of decision trees. I found that swapping out SibSp for Embarked, and noticed a flat increase of over 4% on its accuracy. Due to the incompleteness of several of the variables included in the training data, the inclusion of some attributes were limited due to the nature of the competition and my shortcomings in my knowledge. Both of these are appropriates areas of improvement.

The original score of the default program was 0.7420382165605095. With my contributions, it was improved to 0.77751. This equated to a 4.78% increase in effectiveness.

## Sources

Data and Code for the Kaggle competition was provided by [Kaggle](https://www.kaggle.com/c/titanic) itself.
Website created using website templates released under the [MIT](https://github.com/wowchemy/wowchemy-hugo-modules/blob/master/LICENSE.md) license.
